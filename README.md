# Google Analytics Customer Revenue Prediction

### Business Problem:
 As for every business, the number of customer that generate revenue will be far less than the total customers that the business interacts with. So for every business its really important to understand, analyse and predict the areas of its revenue generation.
 
##### Kaggle Challenge Description:
visit: [Google Analytics Customer Revenue Prediction](https://www.kaggle.com/c/ga-customer-revenue-prediction/overview "Google Analytics Customer Revenue Prediction")
The 80/20 rule has proven true for many businesses–only a small percentage of customers produce most of the revenue. As such, marketing teams are challenged to make appropriate investments in promotional strategies.

RStudio, the developer of free and open tools for R and enterprise-ready products for teams to scale and share work, has partnered with Google Cloud and Kaggle to demonstrate the business impact that thorough data analysis can have.

In this competition, you’re challenged to analyze a Google Merchandise Store (also known as GStore, where Google swag is sold) customer dataset to predict revenue per customer. Hopefully, the outcome will be more actionable operational changes and a better use of marketing budgets for those companies who choose to use data analysis on top of GA data.

#### Source Of Data & Methodology:


##### Machine Learning Methodology:
In this challenge, we are tasked to predict the revenue generated by each over the period of _**December 1st 2018**_ to _**January 31st 2019**_ by using the historical data from _August 1st 2016_ to _October 15th 2018_. Please note that for hte time period that we are predicting we don't have any data.

##### Data:
We have been provided with `train_v2.csv` and `test_v.csv`. 
`train_v2.csv` - the updated training set - contains user transactions from _August 1st 2016_ to _April 30th 2018_.
`test_v2.csv` - the updated test set - contains user transactions from _May 1st 2018_ to _October 15th 2018_.

##### Objective:
`sample_submission_v2.csv` - a updated sample submission file in the correct format. Contains all fullVisitorIds in test_v2.csv. We have to Predicted LogRevenue column should make forward-looking predictions for each of these **_fullVisitorIds_** for the timeframe of **December 1st 2018** to **January 31st 2019**
##### File Descriptions and Features Provided:

Each .csv files has the data for the transactions for number of store vistis
**fullVisitorId** - A unique identifier for each user of the Google Merchandise Store. Our final submission will be dependent on this and will be used for aggregation.

**channelGrouping** - The channel via which the user came to the Store.

**date** - The date on which the user visited the Store.

**device** - The specifications for the device used to access the Store.(_json column_)

**geoNetwork** - This section contains information about the geography of the user.(_json column_)

**socialEngagementType** - Engagement type, either "Socially Engaged" or "Not Socially Engaged".

**totals** - This section contains aggregate values across the session.(_json column_)

**trafficSource** - This section contains information about the Traffic Source from which the session originated.(_json column_)

**visitId** - An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId.

**visitNumber** - The session number for this user. If this is the first session, then this is set to 1.

**visitStartTime** - The timestamp (expressed as POSIX time).

**hits** - This row and nested fields are populated for any and all types of hits. Provides a record of all page visits.

**customDimensions** - This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set.(_json column_)

_json column_: As we can see, there are few columns that are compressed in json format, we will normalize and convert it to normal feature.

##### Evaluation Metric and Target Variable:
![metric_1](https://i.ibb.co/nrdSBNX/metric-1.png)

For each user we have to find out the aggreagate of all transactions during the test data period and it will be log of the value.

##### Why use log on target variable?
We are asked to use log on the target variable:
![](https://i.ibb.co/4jyvsjn/Bar-Plots-Rev-Non-rev.png)
As we can see from this plot, the data is very highly skewwed. Very small number of customers are actually generating any revenue. Only 1.2193% of the customers are generating any revenue. So we are tasked to use log on the target variable.
[Source](http://onlinestatbook.com/2/transformations/log.html#:~:text=The%20log%20transformation%20can%20be,the%20assumptions%20of%20inferential%20statistics. "LogT ransformations")

Lets look at all the features we get after flatening all the json columns.
![](https://i.ibb.co/FYh7q0q/info.png)

#### Exploratory Data Analysis & Data Pre-processing:
**totals.transactionRevenue** is out target variable. We have 1,708,337 data points in train data. 
Lets look at the univarient plots for target variable
![](https://i.ibb.co/LngbfHZ/target-univariant.png "target variable PDF")

The plot on the right, the PDF of actual target_variable. The plot on the left is the PDF of log(target_variable).

Observations:
The PDF of totals.transactionRevenue, is almost normal, with mean at 17.5.

##### channelGrouping
![](https://i.ibb.co/WGvVqYM/channel-group.png)

This is the bar plots of **channelGrouping** indicating the counts on y_axis.
We can see that there are resonable number of unique categories in this features.

But some of the features contains like about 100+ unique categories. For Ex: device.browsers
![](https://i.ibb.co/2Pqtmsm/device-browser.png)

As we can see there are alot of categories. For this feature we have 129 categories. As we can see in the plot. Only few of the categories are repeating resonable times rest of them are like outliers. Most of them are repeating less than ten times. So we can understand that the categories that are repeating few times are not the reason for generating revenue. 

Since all non revenue generating data points are considered as zero, our model is only expected to predict greater than equal to **0**. 
Let's look at the percent of customers per browser.
![](https://i.ibb.co/MSmYsBD/device-browser-use.png)

As we can see after, Opera its been used by very customers. Hence we only keep the most popular browsers.

```python
popular_browsers = ['Chrome','Safari','Firefox','Internet Explorer','Android Webview','Edge','Samsung Internet','Opera Mini','Safari (in-app)','Opera']
train_df['device.browser'] = train_df['device.browser'].apply(lambda x: x if x in popular_browsers else 'unpopular_browser')
```
With this code we are limiting the number of categories in this feature.
After preprocessing we will be getting the below bar plot of browser counts.
![](https://i.ibb.co/Vgdckrt/device-browser-after-grouping-few-categories.png)
Now we can see that the number of unique browsers has reduced significantly.

Please note that the goal of doing this preprocessing is not to use one hot encoding, but to remove the outliers during the data preprocessing phase. This is only to reduce the cardinality of some of the features.

This will be particullarly helpfull when 




